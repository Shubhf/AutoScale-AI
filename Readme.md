
# AutoScaleAI ğŸš€
*A Lightweight Inference Server with Resource-Aware Scheduling*

AutoScaleAI is a modular Python-based microservice that performs real-time machine learning inference while monitoring system performance. It simulates intelligent auto-scaling behavior by tracking latency, CPU usage, and memory consumption per request â€” mimicking how infrastructure-aware systems like Nutanix dynamically adapt to workload pressure.

---

## ğŸ’¡ Project Highlights

- âœ… **FastAPI-based RESTful server** for ML inference
- âœ… **Pre-trained RandomForest model** on the digits dataset
- âœ… **System monitoring** using `psutil` to track CPU and memory usage
- âœ… **Latency analysis** for every prediction
- âœ… **Auto-scaling logic** simulated via conditional triggers in `autoscaler.py`
- âœ… **Modular architecture** with clear separation of concerns

---

## ğŸ› ï¸ Tech Stack

- **Backend**: Python 3.11, FastAPI, Uvicorn
- **ML Model**: scikit-learn `RandomForestClassifier`
- **Monitoring**: `psutil` (CPU + Memory)
- **Packaging**: `joblib`
- **Serving**: REST API via Uvicorn

---

## ğŸ“¦ Folder Structure

```
AutoScaleAI/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py            # FastAPI API endpoints
â”‚   â”œâ”€â”€ model.py           # Model loading and prediction
â”‚   â”œâ”€â”€ autoscaler.py      # Latency-based scaling logic
â”‚   â”œâ”€â”€ utils.py           # Helper functions (monitoring, formatting)
â”‚   â””â”€â”€ train_model.py     # One-time model training script
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸš€ Getting Started

### 1. Clone the repository
```bash
git clone https://github.com/shubh-garg/AutoScaleAI.git
cd AutoScaleAI
```

### 2. Install dependencies
```bash
pip install -r requirements.txt
```

### 3. Train the model
```bash
python app/train_model.py
```

### 4. Run the FastAPI server
```bash
uvicorn app.main:app --reload
```

Visit [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs) for the Swagger UI.

---

## ğŸ§ª Example API Call

**Endpoint**:
```
POST /predict/
```

**Sample Payload**:
```json
{
  "features": [0.0, 0.0, 10.0, 5.0, 8.0, 3.0, ..., 0.0]
}
```

**Sample Response**:
```json
{
  "prediction": 4,
  "latency": 0.0075,
  "cpu": 13.6,
  "memory": 83.6
}
```

---

## ğŸ’¼ Resume Bullet Points

- Developed a modular FastAPI microservice for AI inference that dynamically reports latency and system usage to simulate resource-aware scheduling.
- Integrated lightweight model serving (`RandomForestClassifier`) with live system telemetry via `psutil`, achieving <10ms latency on local CPU with scaling logic triggers.

---

## ğŸŒŸ Possible Extensions

- Add async queueing / worker threads
- Deploy with Docker and Gunicorn
- Integrate GPU usage metrics (if available)
- Replace RF with ONNX or quantized Transformer model
- Live dashboard with Plotly or Grafana

---

## ğŸ“„ License

MIT Â© 2025 Shubh Garg
